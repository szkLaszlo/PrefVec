# -*- coding: utf-8 -*-
"""SocialAttentionDQN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/intersection_social_dqn.ipynb

# Training a DQN with social attention on `intersection-v0`
"""
import os.path
import warnings

from PrefVeC.other_envs.highway_cl import CumulantWrapper

warnings.simplefilter(action='ignore', category=FutureWarning)
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'rlagents'))
import gym
from highway_env.envs import IntersectionEnv

from rlagents.rl_agents.agents.common.factory import load_agent
from rlagents.rl_agents.trainer.evaluation import Evaluation

sys.path.insert(0, '/content/highway-env/scripts/')

"""## Training

We use a policy architecture based on social attention, see [[Leurent and Mercat, 2019]](https://arxiv.org/abs/1911.12250).

"""


class NewGymWrapper(gym.Wrapper):
    def step(self, action):
        s, r, d, i = self.env.step(action)
        return s, r, d, False, i

    def reset(self, **kwargs):
        s = self.env.reset(**kwargs)
        return s, {}


NUM_EPISODES = 10000
is_evaluation = False
use_cumulant = False

env_configs = {"observation": {"type": "Kinematics",
                               "vehicles_count": 6,
                               "features": ["presence", "presence", "x", "y", "vx", "vy", ],
                               "features_range": {"x": [-100, 100], "y": [-100, 100],
                                                  "vx": [-20, 20], "vy": [-20, 20], },
                               "see_behind": True,
                               "absolute": True,
                               "flatten": False,
                               "normalize": True,
                               "observe_intentions": False,
                               "order": "sorted",
                               },
               "action": {"type": "DiscreteMetaAction", "longitudinal": True, "lateral": False,
                          "acceleration_range": [-6, 3], "target_speeds": [0, 4.5, 9]},
               "controlled_vehicles": 1,
               "policy_frequency": 5,
               "initial_vehicle_count": 3,
               "simulation_frequency": 10,
               "spawn_probability": 0.03,
               "duration": 25,  # [s]
               "offscreen_rendering": True,
               "video": False,
               }
intersection_env_config_default = {'observation': {'type': 'Kinematics', 'vehicles_count': 15,
                                                   'features': ['presence', 'x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'],
                                                   'features_range': {'x': [-100, 100], 'y': [-100, 100],
                                                                      'vx': [-20, 20], 'vy': [-20, 20]},
                                                   'absolute': True, 'flatten': True, 'order': 'shuffled'},
                                   'action': {'type': 'DiscreteMetaAction', 'longitudinal': True, 'lateral': False,
                                              'target_speeds': [0, 4.5, 9]},
                                   'simulation_frequency': 15,
                                   'policy_frequency': 1,
                                   'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',
                                   'screen_width': 600,
                                   'screen_height': 600, 'centering_position': [0.5, 0.6], 'scaling': 7.15,
                                   'show_trajectories': False,
                                   'render_agent': True, 'offscreen_rendering': False, 'manual_control': False,
                                   'real_time_rendering': False,
                                   'duration': 13, 'destination': 'o1', 'controlled_vehicles': 1,
                                   'initial_vehicle_count': 3,
                                   'spawn_probability': 0.03, 'collision_reward': -5, 'high_speed_reward': 1,
                                   'arrived_reward': 1,
                                   'reward_speed_range': [7.0, 9.0], 'normalize_reward': False,
                                   'offroad_terminal': False,
                                   'id': 'intersection-v1'}

agent_config_default = {'model': {'type': 'EgoAttentionNetwork', 'layers': [128, 128],
                                  'embedding_layer': {'type': 'MultiLayerPerceptron', 'layers': [64, 64],
                                                      'reshape': False, 'in': len(
                                          intersection_env_config_default.get("observation").get("features")),
                                                      'activation': 'RELU', 'out': None},
                                  'others_embedding_layer': {'type': 'MultiLayerPerceptron', 'layers': [64, 64],
                                                             'reshape': False, 'in': len(
                                          intersection_env_config_default.get("observation").get("features")),
                                                             'activation': 'RELU',
                                                             'out': None}, 'self_attention_layer': None,
                                  'attention_layer': {'type': 'EgoAttention', 'feature_size': 64, 'heads': 2,
                                                      'dropout_factor': 0},
                                  'output_layer': {'type': 'MultiLayerPerceptron', 'layers': [64, 64], 'reshape': False,
                                                   'in': 64, 'out': 3, 'activation': 'RELU'}, 'in': 105, 'out': 3,
                                  'presence_feature_idx': 0},
                        'optimizer': {'type': 'ADAM', 'lr': 0.0005, 'weight_decay': 0, 'k': 5}, 'loss_function': 'l2',
                        'memory_capacity': 15000, 'batch_size': 64, 'gamma': 0.95, 'device': 'cuda:best',
                        'exploration': {'method': 'EpsilonGreedy', 'tau': 15000, 'temperature': 1.0,
                                        'final_temperature': 0.05}, 'target_update': 512, 'double': True,
                        '__class__': "<class 'rlagents.rl_agents.agents.deep_q_network.pytorch.DQNAgent'>",
                        'n_steps': 1}
if use_cumulant:
    env = CumulantWrapper(default_w=[-1000, 1], config=env_configs, go_straight=False, observed_timesteps=1)
else:
    env = IntersectionEnv(intersection_env_config_default)
env = NewGymWrapper(env)

"""Start training. This should take about an hour."""
if not is_evaluation:
    agent = load_agent(agent_config_default, env)
    evaluation = Evaluation(env, agent, num_episodes=NUM_EPISODES, display_env=False, display_agent=False)
    evaluation.train()

# Progress can be visualised in the tensorboard cell above, which should update every 30s (or manually).
# You may need to click the *Fit domain to data* buttons below each graph.

# Testing

else:
    env.config["offscreen_rendering"] = False
    env.rendering = True
    agent = load_agent(agent_config_default, env)
    if use_cumulant:
        recover = ''
    else:
        recover = ''
    evaluation = Evaluation(env, agent, num_episodes=10, recover=recover, )
    evaluation.test()
